{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "from copy import deepcopy\n",
    "from gym import make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "GAMMA = 0.99\n",
    "\n",
    "MEMORY_SIZE = int(1e5)\n",
    "BATCH_SIZE = 512\n",
    "MAX_STEPS = int(1e3)\n",
    "\n",
    "LR_ACTOR =  2e-4\n",
    "LR_CRITIC = 2e-3\n",
    "TAU = 1e-3\n",
    "\n",
    "EPSILON_MAX = 1.0\n",
    "EPSILON_MIN = 1e-3\n",
    "EPSILON_DECAY = 1e-3\n",
    "\n",
    "ENV = 'MountainCarContinuous-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ И КЛАССЫ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "Transition = namedtuple('Transition',\n",
    "    ['state', 'action', 'next_state', 'reward', 'done']\n",
    ")\n",
    "\n",
    "\n",
    "def OUNoise(size, mu=0, theta=0.3, sigma=0.6):\n",
    "    state = mu * np.ones(size)\n",
    "    while True:\n",
    "        yield state\n",
    "        state += -theta * state + sigma * np.array([np.random.randn() for i in range(size)])\n",
    "\n",
    "        \n",
    "def fanin_init(size, fanin=None):\n",
    "    fanin = fanin or size[0]\n",
    "    v = 1. / np.sqrt(fanin)\n",
    "    return torch.Tensor(size).uniform_(-v, v)\n",
    "\n",
    "\n",
    "class ReplayMemory(deque):\n",
    "    def get_batch(self):\n",
    "        def wrap(x):\n",
    "            return torch.tensor(x).to(device)\n",
    "        \n",
    "        \n",
    "        transitions = random.sample(self, BATCH_SIZE)\n",
    "        return Transition(\n",
    "            *map(wrap, zip(*transitions))\n",
    "        )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### РЕАЛИЗАЦИЯ DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, fc1_units=32, fc2_units=32):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_dim)\n",
    "\n",
    "        #self._init_weights()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, fc1_units=16, fc2_units=32):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units + action_dim, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "\n",
    "        #self._init_weights()\n",
    "\n",
    "\n",
    "    def forward(self, x_s, x_a):\n",
    "        x = F.relu(self.fc1(x_s))\n",
    "        x = torch.cat((x, x_a), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_dim, action_dim, soft_update_coef=TAU):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = GAMMA\n",
    "        self.tau = soft_update_coef\n",
    "\n",
    "        self.memory = ReplayMemory(maxlen=MEMORY_SIZE)\n",
    "\n",
    "        self.actor_local = Actor(state_dim, action_dim).to(device)\n",
    "        self.actor_target = deepcopy(self.actor_local)\n",
    "        self.actor_target.eval()\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        self.critic_local = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = deepcopy(self.critic_local)\n",
    "        self.critic_target.eval()\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return 0, 0\n",
    "        \n",
    "        self.actor_local.train()\n",
    "        self.critic_local.train()\n",
    "\n",
    "        batch = self.memory.get_batch()\n",
    "        state_b = batch.state.float()\n",
    "        action_b = batch.action.float()\n",
    "        next_state_b = batch.next_state.float()\n",
    "        reward_b = batch.reward.unsqueeze(1)\n",
    "        done_b = batch.done.unsqueeze(1)\n",
    "        \n",
    "        # ------------------------ critic update ------------------------ # \n",
    "        next_action_b = self.actor_target(next_state_b).detach()\n",
    "        Q_next = self.critic_target(next_state_b, next_action_b)\n",
    "        Q_target = reward_b + self.gamma * Q_next * (~done_b).float()      \n",
    "        Q_predicted = self.critic_local(state_b, action_b)\n",
    "    \n",
    "        critic_loss = F.mse_loss(Q_predicted, Q_target)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self._clamp_parameters(self.critic_local.parameters())\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ------------------------ actor update ------------------------ #\n",
    "        actor_loss = -self.critic_local(state_b, self.actor_local(state_b)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self._clamp_parameters(self.actor_local.parameters())\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.update_targets()\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "    \n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float).to(device).unsqueeze(0)\n",
    "        action = self.actor_local(state).detach()\n",
    "\n",
    "        return action.cpu().data.numpy()\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.actor_local.state_dict(), 'actor.pkl')\n",
    "        torch.save(self.critic_local.state_dict(), 'critic.pkl')\n",
    "\n",
    "\n",
    "    def update_targets(self):\n",
    "        self._soft_update_target(self.actor_target, self.actor_local)\n",
    "        self._soft_update_target(self.critic_target, self.critic_local)\n",
    "        self.actor_target.eval()\n",
    "        self.critic_target.eval()\n",
    "\n",
    "\n",
    "    def _clamp_parameters(self, params):\n",
    "        for param in params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-1.0, 1.0)\n",
    "\n",
    "\n",
    "    def _soft_update_target(self, target_model, local_model):\n",
    "        for t_param, l_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            t_param.data.copy_(self.tau * l_param.data + (1.0 - self.tau) * t_param.data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, agent, env=ENV, n_train_episodes=200, n_test_episodes=100):\n",
    "        self.env = make('MountainCarContinuous-v0')\n",
    "        self.env.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        torch.manual_seed(SEED)\n",
    "        \n",
    "        self.agent = agent\n",
    "        self.n_train = n_train_episodes\n",
    "        self.n_test = n_test_episodes\n",
    "        self.eps = EPSILON_MAX\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        scores = []\n",
    "        prev_reward = float('-inf')\n",
    "        mean_reward = saved_mean = 0\n",
    "        \n",
    "        for episode in range(self.n_train):\n",
    "            state = np.array(self.env.reset(), dtype=np.float32).squeeze()\n",
    "\n",
    "            actor_loss = critic_loss = 0\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            noise = OUNoise(self.agent.action_dim)\n",
    "            self.eps = max(self.eps - EPSILON_DECAY, EPSILON_MIN)\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.agent.act(state) + self.eps * next(noise)\n",
    "                action = np.clip(action, -1.0, 1.0)\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = np.array(next_state, dtype=np.float32).squeeze()\n",
    "                total_reward += reward\n",
    "\n",
    "                self.agent.memory.append(\n",
    "                    Transition(state, action[0], next_state, reward, done)\n",
    "                )\n",
    "                state = next_state\n",
    "\n",
    "                loss_a, loss_c = self.agent.update()\n",
    "                actor_loss += loss_a\n",
    "                critic_loss += loss_c\n",
    "\n",
    "                steps += 1\n",
    "                if steps > MAX_STEPS:\n",
    "                    break\n",
    "\n",
    "            scores.append(total_reward)\n",
    "            if len(scores) > 10:\n",
    "                mean_reward = np.mean(scores[-10:])\n",
    "\n",
    "            if total_reward > 90.0 and mean_reward > max(saved_mean, 90.0):\n",
    "                    saved_mean = mean_reward\n",
    "                    self.agent.save()\n",
    "                    print('################## -- SAVED -- ##################')\n",
    "\n",
    "            print(f'episode #{episode} with reward {total_reward}')\n",
    "            if len(scores) > 10:\n",
    "                print(f'mean reward for last 10 episodes = {mean_reward}', end='\\n\\n')\n",
    "       \n",
    "        return scores\n",
    "                \n",
    "                \n",
    "    def test(self):\n",
    "        scores = []\n",
    "        self.agent.actor_local.load_state_dict(torch.load('actor.pkl'))\n",
    "        self.agent.actor_local.eval()\n",
    "        self.agent.actor_local.to(device)\n",
    "        for ep in range(self.n_test):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                if ep > self.n_test - 10:\n",
    "                    self.env.render()\n",
    "\n",
    "                state = np.array(state, dtype=np.float).squeeze()\n",
    "                action = self.agent.act(state)\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                \n",
    "            self.env.close()\n",
    "            scores.append(total_reward)\n",
    "\n",
    "        print(f'mean reward for {self.n_test} episodes = ', np.mean(scores))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создадим агента и игру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_dim=2, action_dim=1)\n",
    "game = Game(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode #0 with reward -42.09461905269763\n",
      "episode #1 with reward -44.84877907005654\n",
      "episode #2 with reward 86.79760310494125\n",
      "episode #3 with reward 68.41251723339296\n",
      "episode #4 with reward -48.62084215626104\n",
      "episode #5 with reward 85.15153302711325\n",
      "episode #6 with reward 87.40862555816467\n",
      "episode #7 with reward 51.49246529327772\n",
      "episode #8 with reward -53.99790043590273\n",
      "episode #9 with reward -56.90320722934984\n",
      "episode #10 with reward -58.011960851086435\n",
      "mean reward for last 10 episodes = 11.688005447423327\n",
      "\n",
      "episode #11 with reward -48.93336442178366\n",
      "mean reward for last 10 episodes = 11.279546912250614\n",
      "\n",
      "episode #12 with reward 67.78956881636711\n",
      "mean reward for last 10 episodes = 9.3787434833932\n",
      "\n",
      "episode #13 with reward -59.364648811318396\n",
      "mean reward for last 10 episodes = -3.3989731210779346\n",
      "\n",
      "episode #14 with reward 68.84466396737292\n",
      "mean reward for last 10 episodes = 8.34757749128546\n",
      "\n",
      "episode #15 with reward 80.85195373884841\n",
      "mean reward for last 10 episodes = 7.917619562458978\n",
      "\n",
      "episode #16 with reward 73.47542480986395\n",
      "mean reward for last 10 episodes = 6.524299487628906\n",
      "\n",
      "episode #17 with reward 82.06180608410999\n",
      "mean reward for last 10 episodes = 9.581233566712132\n",
      "\n",
      "episode #18 with reward 92.11438340814519\n",
      "mean reward for last 10 episodes = 24.192461951116925\n",
      "\n",
      "episode #19 with reward 72.21496899976535\n",
      "mean reward for last 10 episodes = 37.10427957402844\n",
      "\n",
      "episode #20 with reward 86.16499812797674\n",
      "mean reward for last 10 episodes = 51.521975471934766\n",
      "\n",
      "episode #21 with reward 58.04336663863857\n",
      "mean reward for last 10 episodes = 62.219648577976976\n",
      "\n",
      "episode #22 with reward -53.325119074515946\n",
      "mean reward for last 10 episodes = 50.10817978888867\n",
      "\n",
      "episode #23 with reward 76.22034447554239\n",
      "mean reward for last 10 episodes = 63.666679117574766\n",
      "\n",
      "episode #24 with reward 73.61707261152804\n",
      "mean reward for last 10 episodes = 64.14391998199028\n",
      "\n",
      "episode #25 with reward -68.19823508783976\n",
      "mean reward for last 10 episodes = 49.238901099321446\n",
      "\n",
      "episode #26 with reward -70.15114683722548\n",
      "mean reward for last 10 episodes = 34.8762439346125\n",
      "\n",
      "episode #27 with reward -67.19685602274075\n",
      "mean reward for last 10 episodes = 19.95037772392743\n",
      "\n",
      "episode #28 with reward 41.94683631718932\n",
      "mean reward for last 10 episodes = 14.933623014831847\n",
      "\n",
      "episode #29 with reward 72.13228938978331\n",
      "mean reward for last 10 episodes = 14.925355053833645\n",
      "\n",
      "episode #30 with reward 40.59101987613996\n",
      "mean reward for last 10 episodes = 10.367957228649965\n",
      "\n",
      "episode #31 with reward 56.8701962116437\n",
      "mean reward for last 10 episodes = 10.250640185950477\n",
      "\n",
      "episode #32 with reward 67.63548014096543\n",
      "mean reward for last 10 episodes = 22.346700107498613\n",
      "\n",
      "episode #33 with reward -65.83755403195346\n",
      "mean reward for last 10 episodes = 8.140910256749033\n",
      "\n",
      "episode #34 with reward -66.65924887090557\n",
      "mean reward for last 10 episodes = -5.886721891494332\n",
      "\n",
      "episode #35 with reward 80.44106090912389\n",
      "mean reward for last 10 episodes = 8.977207708202034\n",
      "\n",
      "episode #36 with reward -70.25933667714938\n",
      "mean reward for last 10 episodes = 8.966388724209645\n",
      "\n",
      "episode #37 with reward -69.75113706281036\n",
      "mean reward for last 10 episodes = 8.710960620202687\n",
      "\n",
      "episode #38 with reward 59.583161156303646\n",
      "mean reward for last 10 episodes = 10.474593104114117\n",
      "\n",
      "episode #39 with reward -65.6326738017063\n",
      "mean reward for last 10 episodes = -3.3019032150348435\n",
      "\n",
      "episode #40 with reward -64.6962141785679\n",
      "mean reward for last 10 episodes = -13.830626620505631\n",
      "\n",
      "episode #41 with reward -69.74517873559824\n",
      "mean reward for last 10 episodes = -26.492164115229826\n",
      "\n",
      "episode #42 with reward 69.52673738270497\n",
      "mean reward for last 10 episodes = -26.30303839105587\n",
      "\n",
      "episode #43 with reward 54.29970994030535\n",
      "mean reward for last 10 episodes = -14.289311993829994\n",
      "\n",
      "episode #44 with reward 74.47756753253854\n",
      "mean reward for last 10 episodes = -0.17563035348558032\n",
      "\n",
      "episode #45 with reward 84.61775555255551\n",
      "mean reward for last 10 episodes = 0.24203911085758278\n",
      "\n",
      "episode #46 with reward 86.62186603394365\n",
      "mean reward for last 10 episodes = 15.930159381966883\n",
      "\n",
      "episode #47 with reward 63.5263915376605\n",
      "mean reward for last 10 episodes = 29.25791224201397\n",
      "\n",
      "episode #48 with reward 82.62957153973659\n",
      "mean reward for last 10 episodes = 31.562553280357257\n",
      "\n",
      "episode #49 with reward 83.76490867737047\n",
      "mean reward for last 10 episodes = 46.50231152826494\n",
      "\n",
      "episode #50 with reward 81.00148543204769\n",
      "mean reward for last 10 episodes = 61.0720814893265\n",
      "\n",
      "episode #51 with reward 80.22436259165372\n",
      "mean reward for last 10 episodes = 76.0690356220517\n",
      "\n",
      "episode #52 with reward 82.72454177864364\n",
      "mean reward for last 10 episodes = 77.38881606164557\n",
      "\n",
      "episode #53 with reward 92.1776449998438\n",
      "mean reward for last 10 episodes = 81.17660956759941\n",
      "\n",
      "episode #54 with reward 88.23203567180715\n",
      "mean reward for last 10 episodes = 82.55205638152627\n",
      "\n",
      "episode #55 with reward 77.78286349884873\n",
      "mean reward for last 10 episodes = 81.8685671761556\n",
      "\n",
      "episode #56 with reward 90.96975739844507\n",
      "mean reward for last 10 episodes = 82.30335631260574\n",
      "\n",
      "episode #57 with reward 71.60987459964511\n",
      "mean reward for last 10 episodes = 83.1117046188042\n",
      "\n",
      "episode #58 with reward 87.35783908556473\n",
      "mean reward for last 10 episodes = 83.58453137338701\n",
      "\n",
      "episode #59 with reward -43.25247508869039\n",
      "mean reward for last 10 episodes = 70.88279299678092\n",
      "\n",
      "episode #60 with reward 84.4231270691374\n",
      "mean reward for last 10 episodes = 71.22495716048991\n",
      "\n",
      "episode #61 with reward 88.57800091388522\n",
      "mean reward for last 10 episodes = 72.06032099271306\n",
      "\n",
      "episode #62 with reward 80.03701077234123\n",
      "mean reward for last 10 episodes = 71.79156789208281\n",
      "\n",
      "episode #63 with reward 80.69903583934887\n",
      "mean reward for last 10 episodes = 70.64370697603331\n",
      "\n",
      "episode #64 with reward 86.70808747776172\n",
      "mean reward for last 10 episodes = 70.49131215662877\n",
      "\n",
      "episode #65 with reward 69.40266444197647\n",
      "mean reward for last 10 episodes = 69.65329225094156\n",
      "\n",
      "episode #66 with reward 86.84029999778947\n",
      "mean reward for last 10 episodes = 69.24034651087598\n",
      "\n",
      "episode #67 with reward 90.10601721200553\n",
      "mean reward for last 10 episodes = 71.08996077211202\n",
      "\n",
      "episode #68 with reward 73.48928740307429\n",
      "mean reward for last 10 episodes = 69.703105603863\n",
      "\n",
      "episode #69 with reward 79.5523786239639\n",
      "mean reward for last 10 episodes = 81.98359097512841\n",
      "\n",
      "episode #70 with reward 71.29193919148196\n",
      "mean reward for last 10 episodes = 80.67047218736289\n",
      "\n",
      "episode #71 with reward 83.6739207415529\n",
      "mean reward for last 10 episodes = 80.18006417012963\n",
      "\n",
      "episode #72 with reward 84.45843027441063\n",
      "mean reward for last 10 episodes = 80.62220612033659\n",
      "\n",
      "episode #73 with reward 84.51336619727353\n",
      "mean reward for last 10 episodes = 81.00363915612903\n",
      "\n",
      "episode #74 with reward 88.99536724342715\n",
      "mean reward for last 10 episodes = 81.23236713269559\n",
      "\n",
      "episode #75 with reward 87.8570709931362\n",
      "mean reward for last 10 episodes = 83.07780778781157\n",
      "\n",
      "episode #76 with reward 86.07375908695616\n",
      "mean reward for last 10 episodes = 83.00115369672821\n",
      "\n",
      "episode #77 with reward 84.04774310858176\n",
      "mean reward for last 10 episodes = 82.39532628638585\n",
      "\n",
      "episode #78 with reward 87.02201557855892\n",
      "mean reward for last 10 episodes = 83.74859910393431\n",
      "\n",
      "episode #79 with reward 89.47929593709841\n",
      "mean reward for last 10 episodes = 84.74129083524777\n",
      "\n",
      "episode #80 with reward 90.21851165555455\n",
      "mean reward for last 10 episodes = 86.633948081655\n",
      "\n",
      "episode #81 with reward 89.80502742829773\n",
      "mean reward for last 10 episodes = 87.2470587503295\n",
      "\n",
      "episode #82 with reward 89.38839395776044\n",
      "mean reward for last 10 episodes = 87.74005511866449\n",
      "\n",
      "episode #83 with reward 83.20663734093696\n",
      "mean reward for last 10 episodes = 87.60938223303081\n",
      "\n",
      "episode #84 with reward 77.14639561612255\n",
      "mean reward for last 10 episodes = 86.42448507030035\n",
      "\n",
      "episode #85 with reward 90.97972684304243\n",
      "mean reward for last 10 episodes = 86.73675065529099\n",
      "\n",
      "episode #86 with reward 85.75370225451427\n",
      "mean reward for last 10 episodes = 86.7047449720468\n",
      "\n",
      "episode #87 with reward 83.34070327366767\n",
      "mean reward for last 10 episodes = 86.63404098855538\n",
      "\n",
      "episode #88 with reward 84.39936183456264\n",
      "mean reward for last 10 episodes = 86.37177561415578\n",
      "\n",
      "episode #89 with reward 90.05415911452394\n",
      "mean reward for last 10 episodes = 86.42926193189832\n",
      "\n",
      "episode #90 with reward 87.3640422373857\n",
      "mean reward for last 10 episodes = 86.14381499008144\n",
      "\n",
      "episode #91 with reward 89.73841990003801\n",
      "mean reward for last 10 episodes = 86.13715423725544\n",
      "\n",
      "episode #92 with reward 90.59668098680531\n",
      "mean reward for last 10 episodes = 86.25798294015995\n",
      "\n",
      "episode #93 with reward 92.84220505559685\n",
      "mean reward for last 10 episodes = 87.22153971162594\n",
      "\n",
      "episode #94 with reward 94.28598408310437\n",
      "mean reward for last 10 episodes = 88.93549855832413\n",
      "\n",
      "episode #95 with reward 87.50172123649085\n",
      "mean reward for last 10 episodes = 88.58769799766897\n",
      "\n",
      "episode #96 with reward 94.06084084185376\n",
      "mean reward for last 10 episodes = 89.41841185640291\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #97 with reward 90.56011758666992\n",
      "mean reward for last 10 episodes = 90.14035328770312\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #98 with reward 94.14830052932578\n",
      "mean reward for last 10 episodes = 91.11524715717945\n",
      "\n",
      "episode #99 with reward 82.78071513404467\n",
      "mean reward for last 10 episodes = 90.38790275913152\n",
      "\n",
      "episode #100 with reward 92.78024195321765\n",
      "mean reward for last 10 episodes = 90.92952273071471\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #101 with reward 93.20604277101474\n",
      "mean reward for last 10 episodes = 91.27628501781238\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #102 with reward 94.32866144039475\n",
      "mean reward for last 10 episodes = 91.64948306317133\n",
      "\n",
      "episode #103 with reward 92.1321572834926\n",
      "mean reward for last 10 episodes = 91.57847828596091\n",
      "\n",
      "episode #104 with reward 94.84414819457872\n",
      "mean reward for last 10 episodes = 91.63429469710834\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #105 with reward 90.03010661239811\n",
      "mean reward for last 10 episodes = 91.88713323469906\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #106 with reward 94.35340506683815\n",
      "mean reward for last 10 episodes = 91.91638965719751\n",
      "\n",
      "episode #107 with reward 86.84048790048323\n",
      "mean reward for last 10 episodes = 91.54442668857885\n",
      "\n",
      "episode #108 with reward 93.36787703470412\n",
      "mean reward for last 10 episodes = 91.46638433911667\n",
      "\n",
      "episode #109 with reward 88.72200233524065\n",
      "mean reward for last 10 episodes = 92.06051305923627\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #110 with reward 94.59079815099463\n",
      "mean reward for last 10 episodes = 92.24156867901397\n",
      "\n",
      "episode #111 with reward 88.41267986630143\n",
      "mean reward for last 10 episodes = 91.76223238854263\n",
      "\n",
      "episode #112 with reward 89.09043024185277\n",
      "mean reward for last 10 episodes = 91.23840926868846\n",
      "\n",
      "episode #113 with reward 93.44004063628799\n",
      "mean reward for last 10 episodes = 91.36919760396799\n",
      "\n",
      "episode #114 with reward 93.27367350477223\n",
      "mean reward for last 10 episodes = 91.21215013498734\n",
      "\n",
      "episode #115 with reward 93.52455189789968\n",
      "mean reward for last 10 episodes = 91.56159466353748\n",
      "\n",
      "episode #116 with reward 87.40220453351739\n",
      "mean reward for last 10 episodes = 90.86647461020542\n",
      "\n",
      "episode #117 with reward 94.12411463792242\n",
      "mean reward for last 10 episodes = 91.59483728394932\n",
      "\n",
      "episode #118 with reward 90.37797767890815\n",
      "mean reward for last 10 episodes = 91.29584734836973\n",
      "\n",
      "episode #119 with reward 88.260950308321\n",
      "mean reward for last 10 episodes = 91.24974214567776\n",
      "\n",
      "episode #120 with reward 88.74745899423822\n",
      "mean reward for last 10 episodes = 90.66540823000211\n",
      "\n",
      "episode #121 with reward 93.5924042469657\n",
      "mean reward for last 10 episodes = 91.18338066806855\n",
      "\n",
      "episode #122 with reward 93.02114482050187\n",
      "mean reward for last 10 episodes = 91.57645212593346\n",
      "\n",
      "episode #123 with reward 89.33096253992478\n",
      "mean reward for last 10 episodes = 91.16554431629714\n",
      "\n",
      "episode #124 with reward 92.87173780907948\n",
      "mean reward for last 10 episodes = 91.12535074672788\n",
      "\n",
      "episode #125 with reward 94.07664131299256\n",
      "mean reward for last 10 episodes = 91.18055968823717\n",
      "\n",
      "episode #126 with reward 93.6783766112309\n",
      "mean reward for last 10 episodes = 91.8081768960085\n",
      "\n",
      "episode #127 with reward 94.45962305799664\n",
      "mean reward for last 10 episodes = 91.84172773801592\n",
      "\n",
      "episode #128 with reward 89.9818713096815\n",
      "mean reward for last 10 episodes = 91.80211710109327\n",
      "\n",
      "episode #129 with reward 88.1439025171276\n",
      "mean reward for last 10 episodes = 91.79041232197393\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #130 with reward 93.52863063519497\n",
      "mean reward for last 10 episodes = 92.2685294860696\n",
      "\n",
      "episode #131 with reward 90.80782726958715\n",
      "mean reward for last 10 episodes = 91.99007178833175\n",
      "\n",
      "episode #132 with reward 91.6433379168243\n",
      "mean reward for last 10 episodes = 91.852291097964\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #133 with reward 94.04845294697607\n",
      "mean reward for last 10 episodes = 92.32404013866912\n",
      "\n",
      "episode #134 with reward 92.70119651799517\n",
      "mean reward for last 10 episodes = 92.30698600956069\n",
      "\n",
      "episode #135 with reward 89.38224411497974\n",
      "mean reward for last 10 episodes = 91.8375462897594\n",
      "\n",
      "episode #136 with reward 94.01796596164661\n",
      "mean reward for last 10 episodes = 91.87150522480097\n",
      "\n",
      "episode #137 with reward 94.12985803235448\n",
      "mean reward for last 10 episodes = 91.83852872223676\n",
      "\n",
      "episode #138 with reward 93.07871506890086\n",
      "mean reward for last 10 episodes = 92.1482130981587\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #139 with reward 91.03230817411257\n",
      "mean reward for last 10 episodes = 92.43705366385718\n",
      "\n",
      "episode #140 with reward 89.71900363367952\n",
      "mean reward for last 10 episodes = 92.05609096370566\n",
      "\n",
      "episode #141 with reward 94.01069121630684\n",
      "mean reward for last 10 episodes = 92.37637735837761\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #142 with reward 94.8029781701807\n",
      "mean reward for last 10 episodes = 92.69234138371326\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #143 with reward 94.55105217270776\n",
      "mean reward for last 10 episodes = 92.74260130628642\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #144 with reward 94.26038237004252\n",
      "mean reward for last 10 episodes = 92.89851989149116\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #145 with reward 94.10794559055476\n",
      "mean reward for last 10 episodes = 93.37109003904865\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #146 with reward 94.65242281115722\n",
      "mean reward for last 10 episodes = 93.43453572399972\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #147 with reward 94.53685284026264\n",
      "mean reward for last 10 episodes = 93.47523520479054\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #148 with reward 94.14634518594997\n",
      "mean reward for last 10 episodes = 93.58199821649545\n",
      "\n",
      "episode #149 with reward 89.29762564252349\n",
      "mean reward for last 10 episodes = 93.40852996333653\n",
      "\n",
      "################## -- SAVED -- ##################\n",
      "episode #150 with reward 95.0694408882648\n",
      "mean reward for last 10 episodes = 93.94357368879506\n",
      "\n",
      "episode #151 with reward 93.9508858089889\n",
      "mean reward for last 10 episodes = 93.93759314806327\n",
      "\n",
      "episode #152 with reward 90.5125139265826\n",
      "mean reward for last 10 episodes = 93.50854672370346\n",
      "\n",
      "episode #153 with reward 90.60928400036876\n",
      "mean reward for last 10 episodes = 93.11436990646958\n",
      "\n",
      "episode #154 with reward 87.72878782891837\n",
      "mean reward for last 10 episodes = 92.46121045235716\n",
      "\n",
      "episode #155 with reward 93.8556725910437\n",
      "mean reward for last 10 episodes = 92.43598315240604\n",
      "\n",
      "episode #156 with reward 90.0386020595804\n",
      "mean reward for last 10 episodes = 91.97460107724837\n",
      "\n",
      "episode #157 with reward 92.94788628862449\n",
      "mean reward for last 10 episodes = 91.81570442208454\n",
      "\n",
      "episode #158 with reward 93.65572563512475\n",
      "mean reward for last 10 episodes = 91.76664246700201\n",
      "\n",
      "episode #159 with reward 88.69902109179112\n",
      "mean reward for last 10 episodes = 91.7067820119288\n",
      "\n",
      "episode #160 with reward 90.19708592110362\n",
      "mean reward for last 10 episodes = 91.21954651521267\n",
      "\n",
      "episode #161 with reward 89.61056819386788\n",
      "mean reward for last 10 episodes = 90.78551475370057\n",
      "\n",
      "episode #162 with reward 90.57582417045522\n",
      "mean reward for last 10 episodes = 90.79184577808782\n",
      "\n",
      "episode #163 with reward 94.45286235213833\n",
      "mean reward for last 10 episodes = 91.17620361326479\n",
      "\n",
      "episode #164 with reward 94.73786693061174\n",
      "mean reward for last 10 episodes = 91.87711152343412\n",
      "\n",
      "episode #165 with reward 93.27210431785707\n",
      "mean reward for last 10 episodes = 91.81875469611546\n",
      "\n",
      "episode #166 with reward 94.30682909858555\n",
      "mean reward for last 10 episodes = 92.24557740001597\n",
      "\n",
      "episode #167 with reward 90.08052357405043\n",
      "mean reward for last 10 episodes = 91.95884112855857\n",
      "\n",
      "episode #168 with reward 91.70765824955285\n",
      "mean reward for last 10 episodes = 91.76403439000137\n",
      "\n",
      "episode #169 with reward 94.13366515105331\n",
      "mean reward for last 10 episodes = 92.3074987959276\n",
      "\n",
      "episode #170 with reward 95.42694384494929\n",
      "mean reward for last 10 episodes = 92.83048458831216\n",
      "\n",
      "episode #171 with reward 91.98078571797343\n",
      "mean reward for last 10 episodes = 93.06750634072272\n",
      "\n",
      "episode #172 with reward 91.26020455224275\n",
      "mean reward for last 10 episodes = 93.13594437890147\n",
      "\n",
      "episode #173 with reward 93.99731029449231\n",
      "mean reward for last 10 episodes = 93.09038917313686\n",
      "\n",
      "episode #174 with reward 91.21303844268559\n",
      "mean reward for last 10 episodes = 92.73790632434427\n",
      "\n",
      "episode #175 with reward 91.86686640968534\n",
      "mean reward for last 10 episodes = 92.5973825335271\n",
      "\n",
      "episode #176 with reward 94.36670252986104\n",
      "mean reward for last 10 episodes = 92.60336987665463\n",
      "\n",
      "episode #177 with reward 93.5401344738993\n",
      "mean reward for last 10 episodes = 92.94933096663952\n",
      "\n",
      "episode #178 with reward 94.75536585057093\n",
      "mean reward for last 10 episodes = 93.25410172674133\n",
      "\n",
      "episode #179 with reward 87.37816653142902\n",
      "mean reward for last 10 episodes = 92.5785518647789\n",
      "\n",
      "episode #180 with reward 95.23189724812944\n",
      "mean reward for last 10 episodes = 92.5590472050969\n",
      "\n",
      "episode #181 with reward 94.41202372611689\n",
      "mean reward for last 10 episodes = 92.80217100591128\n",
      "\n",
      "episode #182 with reward 88.20388749148819\n",
      "mean reward for last 10 episodes = 92.4965392998358\n",
      "\n",
      "episode #183 with reward 92.00206252007013\n",
      "mean reward for last 10 episodes = 92.2970145223936\n",
      "\n",
      "episode #184 with reward 94.87878012287027\n",
      "mean reward for last 10 episodes = 92.66358869041204\n",
      "\n",
      "episode #185 with reward 94.28573941282806\n",
      "mean reward for last 10 episodes = 92.90547599072633\n",
      "\n",
      "episode #186 with reward 90.09027617206563\n",
      "mean reward for last 10 episodes = 92.4778333549468\n",
      "\n",
      "episode #187 with reward 90.13060876313688\n",
      "mean reward for last 10 episodes = 92.13688078387054\n",
      "\n",
      "episode #188 with reward 94.12429218683386\n",
      "mean reward for last 10 episodes = 92.07377341749684\n",
      "\n",
      "episode #189 with reward 94.30105785856378\n",
      "mean reward for last 10 episodes = 92.76606255021031\n",
      "\n",
      "episode #190 with reward 94.80154498705784\n",
      "mean reward for last 10 episodes = 92.72302732410316\n",
      "\n",
      "episode #191 with reward 91.40195763566706\n",
      "mean reward for last 10 episodes = 92.42202071505815\n",
      "\n",
      "episode #192 with reward 88.69622001160674\n",
      "mean reward for last 10 episodes = 92.47125396707003\n",
      "\n",
      "episode #193 with reward 94.92304264102559\n",
      "mean reward for last 10 episodes = 92.76335197916558\n",
      "\n",
      "episode #194 with reward 93.74094583111103\n",
      "mean reward for last 10 episodes = 92.64956854998965\n",
      "\n",
      "episode #195 with reward 94.24328280705308\n",
      "mean reward for last 10 episodes = 92.64532288941214\n",
      "\n",
      "episode #196 with reward 90.3825348587015\n",
      "mean reward for last 10 episodes = 92.67454875807574\n",
      "\n",
      "episode #197 with reward 91.93594666776876\n",
      "mean reward for last 10 episodes = 92.85508254853895\n",
      "\n",
      "episode #198 with reward 92.10832465758989\n",
      "mean reward for last 10 episodes = 92.65348579561453\n",
      "\n",
      "episode #199 with reward 94.64277474119869\n",
      "mean reward for last 10 episodes = 92.68765748387801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = game.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot([i + 1 for i in range(0, len(scores), 2)], scores[::2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward for 100 episodes =  93.46120839669643\n"
     ]
    }
   ],
   "source": [
    "game.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
